This project showcases how to design, deploy, and operate a high-availability LLM inference service running on multi-region Kubernetes clusters, with workflow orchestration
